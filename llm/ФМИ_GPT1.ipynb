{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a29f3f8",
   "metadata": {},
   "source": [
    "Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d103b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402e3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functorch import vmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "861d015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class HeadAttention(nn.Module):\n",
    "  def __init__(self, emb_size:int, head_size:int, max_seq_len:int ) -> None:\n",
    "    '''\n",
    "    •\temb_size (тип int) — размерность эмбедингов (позиционных и токинов).\n",
    "    •\thead_size (тип int) — размерность W-матриц (W_k, W_q, W_v).\n",
    "    •\tmax_seq_len (тип int) — максимально возможная длина последовательности.\n",
    "    '''\n",
    "    super().__init__()\n",
    "    self.emb_size = emb_size\n",
    "    self.head_size = head_size\n",
    "    self.max_seq_len = max_seq_len\n",
    "    self.w_k = torch.nn.Linear(emb_size, head_size, bias=False)\n",
    "    self.w_q = torch.nn.Linear(emb_size, head_size, bias=False)\n",
    "    self.w_v = torch.nn.Linear(emb_size, head_size, bias=False)\n",
    "    self.mask_attention = torch.tril(torch.ones(max_seq_len, max_seq_len))\n",
    "\n",
    "  def forward(self, x):\n",
    "    seq_len = len(x[0])\n",
    "    self.key_matrix = self.w_k(x)\n",
    "    self.que_matrix = self.w_q(x)\n",
    "    self.val_matrix = self.w_v(x)\n",
    "\n",
    "\n",
    "    self.att_matrix = torch.matmul(self.que_matrix, self.key_matrix.transpose(1,2))\n",
    "    self.att_matrix /= np.sqrt(self.head_size)\n",
    "    self.sub_mask_matrix = self.mask_attention[:seq_len, :seq_len]\n",
    "    self.att_matrix = torch.where( self.sub_mask_matrix.bool(),  self.att_matrix,\n",
    "        torch.tensor(float('-inf'), device=self.att_matrix.device, dtype=self.att_matrix.dtype)\n",
    "    )\n",
    "    self.att_matrix = torch.softmax(self.att_matrix, dim=2)\n",
    "    self.result_tensor = torch.matmul(self.att_matrix, self.val_matrix)\n",
    "    return self.result_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35620f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads=4\n",
    "emb_size=8\n",
    "head_size=8\n",
    "max_seq_len=24\n",
    "dropout=0.1\n",
    "batch_size=1\n",
    "seq_len = 12\n",
    "\n",
    "t1 = torch.rand(batch_size, seq_len, emb_size)\n",
    "# print(t1)\n",
    "h = HeadAttention(emb_size, head_size, max_seq_len)\n",
    "res = h.forward(t1)\n",
    "# res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b44720e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, num_heads:int, emb_size:int, head_size:int, max_seq_len:int, dropout:float=0.1) -> None:\n",
    "    '''\n",
    "    •\tnum_heads (тип int) — количество голов.\n",
    "    •\temb_size (тип int) — размерность эмбедингов (позиционных и токинов).\n",
    "    •\thead_size (тип int) — размерность W-матриц (W_k, W_q, W_v).\n",
    "    •\tmax_seq_len (тип int) — максимально возможная длина последовательности.\n",
    "    •\tdropout (тип float, от 0.0 до 1.0) — вероятность обнулить значения тензора в слое dropout. Дефолтное значение = 0.1\n",
    "    '''\n",
    "    super().__init__()\n",
    "    self.heads = nn.ModuleList([HeadAttention(emb_size, head_size, max_seq_len) for i in range(num_heads)])\n",
    "    self.l1 = torch.nn.Linear(head_size * num_heads, emb_size)\n",
    "    self.dr = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    res = []\n",
    "    for head in self.heads:\n",
    "      res.append(head.forward(x))\n",
    "    res_tensor = torch.cat(res, dim=2)\n",
    "    return self.dr(self.l1(res_tensor))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "210730d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_heads=4\n",
    "emb_size=8\n",
    "head_size=8\n",
    "max_seq_len=24\n",
    "dropout=0.1\n",
    "batch_size=1\n",
    "seq_len = 12\n",
    "\n",
    "t1 = torch.rand(batch_size, seq_len, emb_size)\n",
    "# print(t1)\n",
    "h = MultiHeadAttention(num_heads, emb_size, head_size, max_seq_len, dropout)\n",
    "res = h.forward(t1)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db05471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, emb_size:int, dropout:float=0.1) -> None:\n",
    "    '''\n",
    "    •\temb_size (тип int) — размерность эмбедингов (позиционных и токинов).\n",
    "    •\tdropout (тип float, от 0.0 до 1.0) — вероятность обнулить значения тензора в слое dropout. Дефолтное значение = 0.1\n",
    "    '''\n",
    "    super().__init__()\n",
    "    # self.l1 = torch.rand(emb_size, emb_size * 4)\n",
    "    self.l1 = torch.nn.Linear(emb_size, emb_size * 4)\n",
    "    self.relu = torch.nn.ReLU()\n",
    "    # self.l2 = torch.rand(emb_size * 4, emb_size)\n",
    "    self.l2 = torch.nn.Linear(emb_size * 4, emb_size)\n",
    "    self.dr = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x = torch.matmul(x, self.l1)\n",
    "    x = self.l1(x)\n",
    "    x = self.relu(x)\n",
    "    # x = torch.matmul(x, self.l2)\n",
    "    x = self.l2(x)\n",
    "    x = self.dr(x)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2087e673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_heads=4\n",
    "emb_size=8\n",
    "head_size=8\n",
    "max_seq_len=24\n",
    "dropout=0.1\n",
    "batch_size=1\n",
    "seq_len = 12\n",
    "\n",
    "t1 = torch.rand(batch_size, seq_len, emb_size)\n",
    "# print(t1)\n",
    "h = FeedForward(emb_size, dropout)\n",
    "res = h.forward(t1)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11199089",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, num_heads:int, emb_size:int, head_size:int, max_seq_len:int, dropout:float=0.1) -> None:\n",
    "    '''\n",
    "    •\tnum_heads (тип int) — количество голов.\n",
    "    •\temb_size (тип int) — размерность эмбедингов (позиционных и токинов).\n",
    "    •\thead_size (тип int) — размерность W-матриц (W_k, W_q, W_v).\n",
    "    •\tmax_seq_len (тип int) — максимально возможная длина последовательности.\n",
    "    •\tdropout (тип float, от 0.0 до 1.0) — вероятность обнулить значения тензора в слое dropout. Дефолтное значение = 0.1\n",
    "    '''\n",
    "    super().__init__()\n",
    "    self.multi_head = MultiHeadAttention(num_heads=num_heads,\n",
    "                        emb_size=emb_size,\n",
    "                        head_size=head_size,\n",
    "                        max_seq_len=max_seq_len,\n",
    "                        dropout=dropout)\n",
    "    self.feed_forward = FeedForward(emb_size=emb_size, dropout=dropout)\n",
    "    self.ln1 = torch.nn.LayerNorm(emb_size)\n",
    "    self.ln2 = torch.nn.LayerNorm(emb_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x += self.multi_head.forward(x)\n",
    "    x = self.ln1(x)\n",
    "    x += self.feed_forward.forward(x)\n",
    "    x = self.ln2(x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75911921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_heads=4\n",
    "emb_size=8\n",
    "head_size=8\n",
    "max_seq_len=24\n",
    "dropout=0.1\n",
    "batch_size=1\n",
    "seq_len = 12\n",
    "\n",
    "\n",
    "decoder = Decoder(num_heads=num_heads,\n",
    "                  emb_size=emb_size,\n",
    "                  head_size=head_size,\n",
    "                  max_seq_len=max_seq_len,\n",
    "                  dropout=dropout)\n",
    "\n",
    "t1 = torch.rand(batch_size, seq_len, emb_size)\n",
    "# print(\"t1\")\n",
    "# print(t1)\n",
    "\n",
    "mh_res = decoder.forward(t1)\n",
    "mh_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1083e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGraph:\n",
    "    _root = None\n",
    "    _all_token_list = []\n",
    "\n",
    "    def clear_root():\n",
    "        CustomGraph._root = None\n",
    "        CustomGraph._all_token_list = []\n",
    "\n",
    "    def __init__(self, value = None, parent = None, parent_sec = None):\n",
    "        if (value == None) & (parent == None) & (parent_sec == None):\n",
    "            CustomGraph.clear_root()\n",
    "            self._root = self\n",
    "            self.type = 0\n",
    "        elif (value != None) & (parent == None) & (parent_sec == None):\n",
    "            self.parent = self._root\n",
    "            self.value = value\n",
    "            self.type = 1\n",
    "            CustomGraph._all_token_list.append(self)\n",
    "        elif (value == None) & (parent != None) & (parent_sec != None):\n",
    "            self.parent = parent\n",
    "            self.parent_sec = parent_sec\n",
    "            self.type = 2\n",
    "            CustomGraph._all_token_list.append(self)\n",
    "        else:\n",
    "            raise Exception(\"Wrong format\")\n",
    "        self.children = {}\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        if cls._root is None:\n",
    "            cls._root = super().__new__(cls)\n",
    "            return cls._root\n",
    "        return super().__new__(cls)\n",
    "    \n",
    "    def __call__(self, *args, **kwds):\n",
    "        if self.type == 0:\n",
    "            return \"\"\n",
    "        elif self.type == 1:\n",
    "            return self.value\n",
    "        elif self.type == 2:\n",
    "            return self.parent() + self.parent_sec()\n",
    "  \n",
    "    def __getitem__(self, key):\n",
    "        if key in self.children:\n",
    "            return self.children.get(key)\n",
    "        elif len(key) > 1:\n",
    "            for i in range(1, len(key)):\n",
    "                if key[:i] in self.children:\n",
    "                    return self.children.get(key[:i])[key]\n",
    "        else:\n",
    "            return self.children.get(key)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        if key in self.children:\n",
    "            return True\n",
    "        elif len(key) > 1:\n",
    "            for i in range(1, len(key)):\n",
    "                if key[:i] in self.children:\n",
    "                    return self.children.get(key[:i])[key]\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.children[key] = value\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self.children) == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            r = 0 if self.type == 0 else 1\n",
    "            for i in self.children.values():\n",
    "                r += len(i)\n",
    "            return r\n",
    "\n",
    "    def __add__(self, value):\n",
    "        self.children[self()+value()] = CustomGraph(parent = self, parent_sec = value)\n",
    "        return self.children[self()+value()] \n",
    "\n",
    "    def __str__(self):\n",
    "        return self()\n",
    "\n",
    "    def longest_child(self):\n",
    "        if len(self.children) == 0:\n",
    "            return 0 \n",
    "        else:\n",
    "            m = 0\n",
    "            for i in self.children.values():\n",
    "                if len(i.children) == 0:\n",
    "                    l = len(i())\n",
    "                else:\n",
    "                    l = i.longest_child()\n",
    "                if l > m:\n",
    "                    m = l\n",
    "            return m\n",
    "\n",
    "    def closest_node(self, key):\n",
    "        # print(f\"key - {key}, self.children - {self.children}\")\n",
    "        if key in self.children:\n",
    "            return self.children.get(key)\n",
    "        elif len(key) > 1:\n",
    "            # print(f\"range(1, len(key)) - {range(1, len(key))}\")\n",
    "            if len(self.children) == 0:\n",
    "                return self\n",
    "            for i in range(1, len(key)):\n",
    "                # print(f\"key[:i] - {key[:i]}\")\n",
    "                if key[:i] in self.children:\n",
    "                    # print(f\"self.children.get(key[:i]) - {self.children.get(key[:i])}, children - {self.children.get(key[:i]).children}\")\n",
    "                    return self.children[key[:i]].closest_node(key)\n",
    "            return self\n",
    "        else:\n",
    "            return self\n",
    "\n",
    "    def all_children(self):\n",
    "        if len(self.children) == 0:\n",
    "            return []\n",
    "        else:\n",
    "            resmas = []\n",
    "            for i in self.children.values():\n",
    "                resmas.append(i())\n",
    "                if len(i.children) != 0:\n",
    "                    resmas += i.all_children()\n",
    "            return resmas\n",
    "\n",
    "    def add(self, value):\n",
    "        if self.type == 0:\n",
    "            if len(value) == 1:\n",
    "                self.children[value] = CustomGraph(value = value)\n",
    "                self.children[value].parent = self\n",
    "            else:\n",
    "                if value in self:\n",
    "                    raise Exception('Token already in graph')\n",
    "                else:\n",
    "                    parent1 = self.closest_node(value)\n",
    "                    # print(f\"parent1 - '{parent1()}, value - {value}'\")\n",
    "                    parent2 = self.closest_node(value[len(parent1()):])\n",
    "                    # print(f\"parent2 - '{parent2()}, value[:len(parent1())] - {value[:len(parent1())]}'\")\n",
    "                    parent1 + parent2\n",
    "        else:\n",
    "            raise Exception(\"This function is availble only for root node\")\n",
    "    \n",
    "    def print_children(self, indent):\n",
    "        print(\" \"*indent, f\"'{self()}'\")\n",
    "        for i in self.children.values():\n",
    "            if i.parent is self:\n",
    "                i.print_children(indent+1)\n",
    "\n",
    "    def print_tree(self):\n",
    "        self._root.print_children(0)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        i = 0\n",
    "        resmas = []\n",
    "        while i < len(text):\n",
    "            # print(i)\n",
    "            max_token_j = 0\n",
    "            max_length = 0\n",
    "            \n",
    "            curnode = self[text[i]]\n",
    "            max_len = curnode.longest_child()\n",
    "            if max_len == 0:\n",
    "                resmas.append(text[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                for j in range(i+1, min(len(text), i+1+max_len)):\n",
    "                    # print(j,  min(len(text), i+max_len))\n",
    "                    if text[i:j] in self:\n",
    "                        # print(f\"sub- {text[i:j]} node - {self.closest_node(text[i:j])} - {self.closest_node(text[i:j])()}\") \n",
    "                        l = len(self.closest_node(text[i:j])())\n",
    "                        if l > max_length:\n",
    "                            max_length = l\n",
    "                            max_token_j = j\n",
    "                resmas.append(text[i:max_token_j])\n",
    "                i = max_token_j\n",
    "            # print(resmas)\n",
    "        return resmas\n",
    "    \n",
    "    def pairing(self, text):\n",
    "        token_list = self.tokenize(text)\n",
    "        # print(f\"token_list - {token_list}\")\n",
    "        pairs_list = [token_list[i] + token_list[i+1] for i in range(len(token_list)-1)]\n",
    "\n",
    "        pairs_dict = {}\n",
    "        max_val = 0\n",
    "        max_ind = \"\"\n",
    "        max_ind_pos = 0\n",
    "        for i, val in enumerate(pairs_list):\n",
    "            pairs_dict[val] = pairs_dict.get(val, [])\n",
    "            pairs_dict[val].append(i)\n",
    "\n",
    "            if (len(pairs_dict[val]) > max_val) or ( len(pairs_dict[val]) == max_val and pairs_dict[val][0] < max_ind_pos[0]) :\n",
    "                max_val = len(pairs_dict[val])\n",
    "                max_ind = val\n",
    "                max_ind_pos = pairs_dict[val]\n",
    "        return max_ind\n",
    "    \n",
    "    def id2token(self):\n",
    "        return { i:j for i,j in enumerate(list(map(str, self._all_token_list)))}\n",
    "    def token2id(self):\n",
    "        return { j:i for i,j in enumerate(list(map(str, self._all_token_list)))}\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e4310ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomGraph at 0x7e0150801110>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = CustomGraph()\n",
    "g.add('a')\n",
    "g.add('b')\n",
    "g.add(' ')\n",
    "g['a'] + g['a']\n",
    "\n",
    "\n",
    "# g['a'].print_tree()\n",
    "# g.tokenize(\"abaa bbbb\")\n",
    "# pairing = g.pairing(\"aa bbbb\")\n",
    "# g.add(pairing)\n",
    "# g.tokenize(\"abaa bbbb\")\n",
    "# print(g.id2token())\n",
    "# print(g.token2id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e25e5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48efc5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 22, 11, 25, 4, 1, 25, 4, 0, 5, 17, 9, 14, 19, 27, 4, 1, 0, 12, 7, 0, 5, 11, 7, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "class BPE():\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.graph_root = CustomGraph()\n",
    "\n",
    "\n",
    "    def fit(self, text:str):\n",
    "        '''\n",
    "        text (тип str) — корпус текста для обучения\n",
    "        '''\n",
    "        token_list = list(sorted(set(text)))\n",
    "        # print(token_list)\n",
    "        for i in token_list:\n",
    "            self.graph_root.add(i)\n",
    "\n",
    "        l1 = len(self.graph_root)\n",
    "        while len(self.graph_root) < self.vocab_size:\n",
    "            # print(self.graph_root.tokenize(text))\n",
    "            pairing = self.graph_root.pairing(text)\n",
    "            if pairing == \"\":\n",
    "                break\n",
    "            # print(f\"pairing - '{pairing}'\")\n",
    "            self.graph_root.add(pairing)\n",
    "            # self.graph_root.print_tree()\n",
    "            if l1 == len(self.graph_root):\n",
    "                raise Exception(\"No new data for tokenizer\")\n",
    "            l1 = len(self.graph_root)\n",
    "        \n",
    "        self.id2token = self.graph_root.id2token()\n",
    "        self.token2id = self.graph_root.token2id()\n",
    "    \n",
    "    def encode(self, text):\n",
    "        tokens = self.graph_root.tokenize(text)\n",
    "        # return tokens\n",
    "        return [self.token2id[token] for token in tokens]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "bpe = BPE(28)\n",
    "# bpe.fit(\"aa bb\")\n",
    "# bpe.fit(\"косил косой косы косиц и искоса косматый космос\")\n",
    "text = 'На дворе дрова, за двором дрова, дрова вширь двора, не вместит двор дров, надо дрова выдворить на дровяной двор.'\n",
    "bpe\n",
    "bpe.fit(text)\n",
    "\n",
    "# print(bpe.id2token)\n",
    "# print(bpe.token2id)\n",
    "# print(list(map(str, bpe.graph_root._all_token_list)))\n",
    "text1 = \"вором дрова, дрова вширь двора, не вмест\"\n",
    "print(bpe.encode(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ca56b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "class BPE():\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "\n",
    "    def fit(self, text:str):\n",
    "        '''\n",
    "        text (тип str) — корпус текста для обучения\n",
    "        '''\n",
    "        token_list = list(sorted(set(text))) # Создали список первичных токенов\n",
    "        text_list = [1] * len(text) # Создали список с длинами токенов\n",
    "\n",
    "        while len(token_list) < self.vocab_size: # Пока не заполнится словарь:\n",
    "\n",
    "            i = 0 # Начинаем новую итерацию с нуля\n",
    "            prev_token = text[i:i + text_list[i]] # Собираем первый токен\n",
    "            prev_token_pos = i\n",
    "            i += text_list[i]\n",
    "\n",
    "            token_dict = {} # Словарь токенов и их позиций\n",
    "            max_tokens_count = 0  # Данные о самом частом токене - количество\n",
    "            max_tokens_val = \"\" # Название\n",
    "            max_tokens_pos = [] # Все позиции\n",
    "\n",
    "            while i < len(text): # Проходим по всему тексту\n",
    "                actual_token = text[i:i + text_list[i]] # Следующий токен\n",
    "                actual_token_pos = i\n",
    "\n",
    "                new_token = prev_token + actual_token # Новый токен из двух предыдущих\n",
    "                new_token_pos = prev_token_pos\n",
    "                token_dict[new_token] = token_dict.get(new_token, []) # Записываем данные о новом токене в словарь\n",
    "                token_dict[new_token].append(new_token_pos)\n",
    "\n",
    "                if len(token_dict[new_token]) > max_tokens_count or ( len(token_dict[new_token]) == max_tokens_count and token_dict[new_token][0] < max_tokens_pos[0] ): # Если выполнены условия\n",
    "                    max_tokens_count = len(token_dict[new_token]) # Запоминаем самый частый токен\n",
    "                    max_tokens_val = new_token\n",
    "                    max_tokens_pos = token_dict[new_token]\n",
    "                \n",
    "                prev_token = actual_token # Актуальный токен -> Старый\n",
    "                prev_token_pos = actual_token_pos\n",
    "                i += text_list[i] # Передвигаем курсор\n",
    "\n",
    "            if max_tokens_val == \"\":\n",
    "                break\n",
    "            token_list.append(max_tokens_val)\n",
    "            for pos in max_tokens_pos: # В списке с длинами токенов меняем значения на позициях с нашим токеном\n",
    "                text_list[pos] = len(max_tokens_val)\n",
    "        self.token_list = token_list\n",
    "        self.id2token = {i:j for i, j in enumerate(token_list)}\n",
    "        self.token2id = {j:i for i, j in enumerate(token_list)}\n",
    "            \n",
    "    \n",
    "    def encode(self, text):\n",
    "        tokens = []\n",
    "        i = 0\n",
    "\n",
    "        while i < len(text):\n",
    "            candidates = []\n",
    "            for token in self.token_list:\n",
    "                if token.startswith(text[i]):\n",
    "                    candidates.append(token)\n",
    "            \n",
    "            last_candidate = candidates[0]\n",
    "            len_last_candidate = len(last_candidate)\n",
    "            for j in range(1, max(map(len, candidates))+1):\n",
    "                t = text[i:i+j]\n",
    "                l = len(t)\n",
    "                if t in candidates and l > len_last_candidate:\n",
    "                    last_candidate = t\n",
    "                    len_last_candidate = l\n",
    "            tokens.append(last_candidate)\n",
    "            i += len(last_candidate)\n",
    "        # return tokens\n",
    "        return [self.token2id[token] for token in tokens]\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        return \"\".join([self.id2token[_id] for _id in token_ids])\n",
    "    \n",
    "    def save(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            dill.dump(self, f)\n",
    "        print(f\"Объект сохранён в {filename}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            obj = dill.load(f)\n",
    "                \n",
    "        print(f\"Объект загружен из {filename}\")\n",
    "        return obj\n",
    "\n",
    "\n",
    "\n",
    "bpe = BPE(28)\n",
    "# bpe.fit(\"aa bb\")\n",
    "# bpe.fit(\"косил косой косы косиц и искоса косматый космос\")\n",
    "text = 'На дворе дрова, за двором дрова, дрова вширь двора, не вместит двор дров, надо дрова выдворить на дровяной двор.'\n",
    "bpe.fit(text)\n",
    "\n",
    "# print(bpe.id2token)\n",
    "# print(bpe.token2id)\n",
    "# print(list(map(str, bpe.graph_root._all_token_list)))\n",
    "# text1 = \"вором дрова, дрова вширь двора, не вмест\"\n",
    "# print(bpe.encode(text1))\n",
    "# tokens = [23, 22, 11, 25, 4, 1, 25, 4, 0, 5, 17, 9, 14, 19, 27, 4, 1, 0, 12, 7, 0, 5, 11, 7, 15, 16]\n",
    "# print(bpe.decode(tokens))\n",
    "\n",
    "\n",
    "# bpe = BPE(vocab_size=1000)\n",
    "# bpe.fit(text)\n",
    "# bpe.save('data/bpe.dill')\n",
    "# bpe = BPE.load('data/bpe.dill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "861d2589",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# bpe = BPE.load('data/bpe.dill')\n",
    "# bpe.token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3a96137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbeddings(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size,  *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.matrix = torch.nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x - batch_size * seq_len\n",
    "        '''\n",
    "        return self.matrix(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41818e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.3527e-01, -8.6134e-02, -5.3151e-01, -6.1255e-01,  7.4053e-01,\n",
       "         -5.1576e-02,  1.2855e+00,  6.6857e-02, -2.0344e+00,  7.1355e-04,\n",
       "          3.0162e-01,  1.2442e+00],\n",
       "        [ 1.9120e+00, -2.5376e+00, -1.0149e+00, -1.0310e+00, -9.7896e-01,\n",
       "         -7.4052e-01,  1.5298e-01,  1.7207e-01, -1.5266e+00,  1.3549e+00,\n",
       "          4.5564e-01,  2.1546e+00],\n",
       "        [ 1.2551e+00, -9.2100e-01, -3.7059e-02,  5.5143e-01,  1.1274e+00,\n",
       "          1.7574e+00, -1.1630e+00,  8.4595e-01, -1.1852e+00, -7.8014e-01,\n",
       "          1.3317e+00, -8.5087e-01],\n",
       "        [-1.5731e+00,  3.3233e-01,  4.1538e-01,  9.0038e-02, -8.2872e-02,\n",
       "         -3.9480e-01,  1.1202e+00, -1.0003e+00, -2.1596e-01, -1.5017e+00,\n",
       "          3.5439e-01,  6.7743e-02],\n",
       "        [-4.7294e-01,  9.0389e-01, -4.5013e-01, -1.8756e-01, -1.6522e+00,\n",
       "         -1.1799e-01, -3.4191e-01, -7.6679e-01, -8.4476e-01, -1.7725e+00,\n",
       "         -1.3085e+00, -4.7036e-01],\n",
       "        [ 6.6501e-01,  4.5917e-01,  7.2080e-01,  1.3233e-01, -3.8343e-01,\n",
       "         -7.7588e-01, -9.8056e-01, -3.4085e-01, -1.0010e+00, -1.2659e+00,\n",
       "          1.3088e+00, -5.8216e-01],\n",
       "        [-1.9118e+00, -1.4917e+00, -9.5631e-01, -1.6004e+00, -4.9403e-01,\n",
       "          1.3455e+00,  5.3849e-02, -7.3361e-01, -1.4242e+00,  1.9480e-01,\n",
       "          2.5328e-01, -1.5678e+00]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionalEmbeddings(torch.nn.Module):\n",
    "    def __init__(self, max_seq_len , emb_size ,  *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.emb_size = emb_size\n",
    "        self.matrix = torch.nn.Embedding(max_seq_len, emb_size)\n",
    "    def forward(self, seq_len ):\n",
    "        '''\n",
    "        x - batch_size * seq_len\n",
    "        '''\n",
    "        return self.matrix.weight[0:seq_len]\n",
    "pe = PositionalEmbeddings(10, 12)\n",
    "pe.forward(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ff4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size: int,\n",
    "                 max_seq_len: int, \n",
    "                 emb_size: int, \n",
    "                 num_heads: int, \n",
    "                 head_size: int, \n",
    "                 num_layers: int, \n",
    "                 dropout: float = 0.1,\n",
    "                 device:str = \"cpu\",\n",
    "                  *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.token_embeddings = TokenEmbeddings(vocab_size, emb_size)\n",
    "        self.positional_embeddings = PositionalEmbeddings(max_seq_len, emb_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.decoders = torch.nn.Sequential(*[Decoder(num_heads, emb_size, head_size, max_seq_len) for i in range(num_layers)])\n",
    "        self.linear = torch.nn.Linear(emb_size, vocab_size)\n",
    "        self.max_seq_len = max_seq_len\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Получает на вход последовательность x (тип int) размером batch_size x seq_len. Где:\n",
    "            batch_size — количество батчей.\n",
    "            seq_len — длина входящей последовательности.\n",
    "        '''\n",
    "        emb_tokens = self.token_embeddings.forward(x)\n",
    "        emb_positi = self.positional_embeddings(x.shape[1])\n",
    "        embedding = emb_tokens + emb_positi\n",
    "        x = self.dropout(embedding)\n",
    "        x = self.decoders(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    def generate(self, x, max_new_tokens ):\n",
    "        for i in range(max_new_tokens):\n",
    "            new_x = x[:, -self.max_seq_len:]\n",
    "            logits = self.forward(new_x)\n",
    "            maxed = torch.softmax(logits[:,-1, :], dim=1)\n",
    "            arg_max = torch.argmax(maxed, dim=1)\n",
    "            arg_max = torch.reshape(arg_max, (maxed.shape[0], 1))\n",
    "            x = torch.concat([x, arg_max], dim=1)\n",
    "        return \n",
    "\n",
    "num_heads=5\n",
    "emb_size=12\n",
    "head_size=8\n",
    "max_seq_len=40\n",
    "dropout=0.1\n",
    "batch_size=2\n",
    "seq_len = 12\n",
    "vocab_size = 15\n",
    "num_layers = 5\n",
    "device = \"gpu\"\n",
    "max_new_tokens = 5\n",
    "\n",
    "gpt = GPT(\n",
    "    num_heads = num_heads,\n",
    "    emb_size = emb_size,\n",
    "    head_size = head_size,\n",
    "    max_seq_len = max_seq_len,\n",
    "    dropout = dropout,\n",
    "    vocab_size = vocab_size,\n",
    "    num_layers = num_layers,\n",
    "    device = device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bb01a412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 13,  9,  2,  0,  9, 11,  1,  6,  9,  7, 10],\n",
      "        [ 3,  0,  8, 10,  1,  7, 11,  0, 13,  2, 11,  4]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[11, 13,  9,  2,  0,  9, 11,  1,  6,  9,  7, 10,  4,  1,  5,  1,  5],\n",
       "        [ 3,  0,  8, 10,  1,  7, 11,  0, 13,  2, 11,  4,  4, 13,  2,  1,  5]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t1 = torch.randint(0, 14, (batch_size, seq_len))\n",
    "print(t1)\n",
    "gpt.generate(t1, max_new_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
